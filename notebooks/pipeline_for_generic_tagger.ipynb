{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipeline for generic tagger**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains a pipeline for developing a historic Dutch token classification model (a tagger).\n",
    "The starting material for this notebook is a dataframe of 350 texts from the dbnl.\n",
    "\n",
    "In the case of this notebook we will work on a animal tagger, but this set-up can be used for multiple other entities.\n",
    "\n",
    "In this notebook we'll do the following things:\n",
    "\n",
    "1. Make a list with animal_names (with generic datasets and word2vec);\n",
    "2. Naively tag sentences with this animal list;\n",
    "3. Remove most frequent homonyms from the animal list and tag again;\n",
    "4. Train a model on the naive tags;\n",
    "5. Let model select sentences with animals; \n",
    "6. Manually tag these sentences in doccano;\n",
    "7. Train a new model on the manually annotated dataset. \n",
    "8. Repeat steps 5-7 till you're satisfied with the results.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    pipeline,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load dataframe with sentences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_path = \"data/dbnl_dfs/total_with_sentences_100.pkl\"\n",
    "df = pd.read_pickle(f_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Create animal list**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.read_csv(\n",
    "    \"data/wnt_exports/animals/gtb-export2.csv\", encoding=\"unicode_escape\", delimiter=\";\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"data/wnt_exports/animals\"\n",
    "csv_files = glob.glob(os.path.join(folder_path, \"*.csv\"))\n",
    "\n",
    "animal_lst = []\n",
    "for file_path in csv_files:\n",
    "    gtb_df = pd.read_csv(file_path, encoding=\"unicode_escape\", delimiter=\";\")\n",
    "    animal_lst += gtb_df[\"Trefwoord\"].tolist()\n",
    "    animal_lst += gtb_df[\"Originele spelling\"].tolist()\n",
    "    animal_lst += gtb_df[\"Betekenis\"].tolist()\n",
    "\n",
    "animal_lst = list(set([string.lower() for string in animal_lst]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean this list manually and save the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/wnt_lsts/animal_lst_from_wnt.pickle\", \"wb\") as f:\n",
    "    pickle.dump(animal_lst, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Enlarge with Word2Vec**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a Dutch word2vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://github.com/coosto/dutch-word-embeddings/releases/download/v1.0/model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.enlarge_reference_lst import word2vec_phishing_expedition\n",
    "\n",
    "path_to_word2vec_model = \"\"\n",
    "similarity_threshold = 0.6\n",
    "with open(\"data/wnt_lsts/animal_lst_from_wnt.pickle\", \"rb\") as f:\n",
    "    animal_lst = pickle.load(f)\n",
    "\n",
    "\n",
    "additions = word2vec_phishing_expedition(\n",
    "    path_to_word2vec_model, animal_lst, similarity_threshold\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then check these additions manually, add them to the list and save."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_animal_lst = animal_lst + additions\n",
    "with open(\"data/wnt_lsts/animal_lst_word2vec_enlarged.pickle\", \"wb\") as f:\n",
    "    pickle.dump(enriched_animal_lst, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tag naively for diagnostic reasons** "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the dataframe with sentences and take a sample from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.read_pickle(\"data/dbnl_dfs/total_with_sentences_100.pkl\")\n",
    "sample_df = total_df.sample(n=100000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the aniamal_lst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/wnt_lsts/animal_lst.pkl\", \"rb\") as file:\n",
    "    animals = pickle.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tag the dataframe naively and examine the most frequent tagged words. Check for homonyms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.tag_sentence import *\n",
    "\n",
    "sample_df[\"tagged\"] = sample_df[\"sentence\"].apply(\n",
    "    lambda x: check_for_organisms(x, animals)\n",
    ")\n",
    "\n",
    "lst_sentence = sample_df[\"sentence\"].tolist()\n",
    "lst_tags = sample_df[\"tagged\"].tolist()\n",
    "sentence_and_tagged = list(zip(lst_sentence, lst_tags))\n",
    "\n",
    "lst = []\n",
    "for sentence, tagged in sentence_and_tagged:\n",
    "    individual_coupled = list(zip(sentence, tagged))\n",
    "    for e in individual_coupled:\n",
    "        if e[1] == 1:\n",
    "            lst.append(e[0])\n",
    "\n",
    "diagnostic_df = pd.DataFrame(lst, columns=[\"tagged_entity\"])\n",
    "animal_counts = diagnostic_df[\"tagged_entity\"].value_counts()\n",
    "animal_counts.to_csv(\"animal_counts.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the homonyms that are frequently present. Save the animal_lst again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_be_removed = [\n",
    "    \"und\",\n",
    "    \"wind\",\n",
    "    \"sterker\",\n",
    "    \"nadere\",\n",
    "    \"wint\",\n",
    "    \"ridder\",\n",
    "    \"monnik\",\n",
    "    \"harder\",\n",
    "    \"tuinkamer\",\n",
    "    \"vacht\",\n",
    "    \"volgeling\",\n",
    "]\n",
    "animals_without_frequent_homonyms = [\n",
    "    animal for animal in animals if animal not in to_be_removed\n",
    "]\n",
    "\n",
    "with open(\"data/wnt_lsts/animal_lst_cleaned_for_homonyms.pkl\", \"wb\") as f:\n",
    "    pickle.dump(animals_without_frequent_homonyms, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Tag naively for training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_to_f = \"data/wnt_lsts/animal_lst_cleaned_for_homonyms.pkl\"\n",
    "with open(p_to_f, \"rb\") as file:\n",
    "    animals = pickle.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select for sentences with animals in it (according to naive tagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.tag_sentence import *\n",
    "\n",
    "total_df = pd.read_pickle(\"data/dbnl_dfs/total_with_sentences_100.pkl\")\n",
    "sample_df = total_df.sample(n=100000)\n",
    "sample_df[\"tagged\"] = sample_df[\"sentence\"].apply(\n",
    "    lambda x: check_for_organisms(x, animals)\n",
    ")\n",
    "sample_df[\"has_ones\"] = sample_df[\"tagged\"].apply(lambda x: 1 in x)\n",
    "only_positives = sample_df.loc[sample_df[\"has_ones\"] == True]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter for Dutch and Afrikaans (historic Dutch can be misclassified as Afrikaans sometimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_284/3012940755.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  only_positives[\"language\"] =only_positives[\"sentence\"].apply(lambda x: find_language(x))\n"
     ]
    }
   ],
   "source": [
    "only_positives[\"language\"] = only_positives[\"sentence\"].apply(\n",
    "    lambda x: find_language(x)\n",
    ")\n",
    "only_dutch_positives = only_positives[only_positives[\"language\"].isin([\"af\", \"nl\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_to_save = only_dutch_positives.drop(\n",
    "    columns=[\"text_id\", \"has_ones\", \"language\"]\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/annotated/naively/2513_sentences.pkl\", \"wb\") as f:\n",
    "    pickle.dump(df_to_save, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"emanjavacas/GysBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-2e2df5eadfb2b146\n",
      "Found cached dataset pandas (/home/arjan_v_d/.cache/huggingface/datasets/pandas/default-2e2df5eadfb2b146/0.0.0/3ac4ffc4563c796122ef66899b9485a3f1a977553e2d2a8a318c72b8cc6f2202)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fcd1d912ed495e8d3fe22a5b87f212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path_to_tagged_file = \"data/annotated/naively/2513_sentences.pkl\"\n",
    "\n",
    "total_ds = load_dataset(\"pandas\", data_files=path_to_tagged_file)[\"train\"]\n",
    "\n",
    "total_ds = total_ds.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e9713fd33534775af2fe4e6b0065995",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e4a6ecc1534fa38867226154262eec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from preprocess.tokenize_and_align import tokenize_and_align_labels\n",
    "\n",
    "train_ds = total_ds[\"train\"].map(tokenize_and_align_labels, batched=True)\n",
    "test_ds = total_ds[\"test\"].map(tokenize_and_align_labels, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_model.metrics import compute_metrics\n",
    "from transformers import DataCollatorForTokenClassification\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = (\"not_an_organism\", \"organism\")\n",
    "tag2id = {tag: id for id, tag in enumerate(unique_tags)}\n",
    "id2tag = {id: tag for tag, id in tag2id.items()}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, id2label=id2tag, label2id=tag2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_for_model = \"is_het_een_dier_v1\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=name_for_model,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,  # ,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Doccano check of model predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from active_learning.from_ner_to_doccano import hf_output_for_doccano\n",
    "\n",
    "model_checkpoint = \"emanjavacas/GysBERT\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_checkpoint, max_length=512, truncation=True, model_max_length=512\n",
    ")\n",
    "classifier = pipeline(\n",
    "    task=\"ner\",\n",
    "    model=\"ArjanvD95/is_het_een_dier_v1\",\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"average\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import sample\n",
    "\n",
    "df_with_sentences = pd.read_csv(\"df_with_sentences.csv\")  # have a look at this\n",
    "sentence_lst = sample(df_with_sentences[\"sentence\"].tolist(), 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_lst = hf_output_for_doccano(sentences=sentence_lst, classifier=classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f_name = \"tuesday_v1\"\n",
    "with open(f\"data/for_doccano/{f_name}\", \"w\") as file:\n",
    "    for obj in json_lst:\n",
    "        json_line = json.dumps(obj)\n",
    "        file.write(json_line + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
