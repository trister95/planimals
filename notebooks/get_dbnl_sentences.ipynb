{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eNUNUYj-w0d"
   },
   "source": [
    "## Clone, install, import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wJk2cZN9lf-",
    "outputId": "d8c74583-e407-46f7-81f5-7ddb2e9e8927",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone #add link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uctodata configuration directory already exists: /home/arjan_v_d/.config/ucto, refusing to overwrite, please remove it first if you want to install all data anew.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt -q\n",
    "\n",
    "import ucto\n",
    "ucto.installdata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "3ghClK8j_CeT"
   },
   "outputs": [],
   "source": [
    "from preprocess.add_html_declaration import *\n",
    "from preprocess.dbnl_xml_to_txt import *\n",
    "#from preprocess.folia_to_df import *\n",
    "import ucto\n",
    "import os\n",
    "#from preprocess.txt_to_folia import *\n",
    "import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxMAqL-Z-_2G",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9ThYhJj-6Ja"
   },
   "outputs": [],
   "source": [
    "from utils.create_if_absent import create_if_absent\n",
    "from download.download_files import download_and_unzip\n",
    "\n",
    "create_if_absent(\"dbnl_xml\")\n",
    "download_and_unzip(url = \"https://www.dbnl.org/letterkunde/pd/xml_pd.zip\", output_dir = \"dbnl_xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UAHKveR_KtO"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "98TjeKar_GKR"
   },
   "outputs": [],
   "source": [
    "html_to_decimal_dict = make_html_entity_dict()\n",
    "addition =  additional_declaration_str(html_to_decimal_dict, '.dtd\"')\n",
    "add_declaration_to_xml('dbnl_xml', '<!DOCTYPE', '.dtd\"', addition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "id": "HcWUpuin_Oqn",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory dbnl_txt created\n",
      "file teen002bijz01 could not be parsed\n",
      "file hugo001piad01 could not be parsed\n",
      "file vaen001quin01 could not be parsed\n",
      "file vry_001anat01 could not be parsed\n",
      "file _gid001183901 could not be parsed\n",
      "file luyk001jezu01 could not be parsed\n",
      "file aa__001biog02 could not be parsed\n",
      "file herl001besc01 could not be parsed\n",
      "file lede003drie01 could not be parsed\n",
      "file elge001zinn01 could not be parsed\n",
      "file ferm001nieu01 could not be parsed\n",
      "file leuv001amor01 could not be parsed\n",
      "file aa__001biog11 could not be parsed\n",
      "file carp002verh01 could not be parsed\n"
     ]
    }
   ],
   "source": [
    "create_if_absent(\"dbnl_txt\")\n",
    "dbnl_to_txt(\"dbnl_xml\", \"dbnl_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "E3OwXGIp_hDZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory dbnl_folia already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "looping through text files: 100%|██████████| 3415/3415 [38:02<00:00,  1.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "create_if_absent(\"dbnl_folia\")\n",
    "txt_to_folia(\"dbnl_txt\", \"dbnl_folia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import folia.main as folia\n",
    "import pandas as pd\n",
    "import os\n",
    "import random \n",
    "from planimals.preprocess.dbnl_xml_to_txt import extract_dbnl_id\n",
    "\n",
    "def list_folia_files(directory):\n",
    "  \"\"\" \n",
    "  List all folia files in a certain directory.\n",
    "  \"\"\"\n",
    "  folia_files = []\n",
    "  for filename in os.listdir(directory):\n",
    "    if filename.endswith('folia.xml'):\n",
    "      folia_files.append(filename)\n",
    "  return folia_files\n",
    "\n",
    "def split_words(sentence):\n",
    "  sentence_lst = []\n",
    "  for w in sentence.words():\n",
    "    sentence_lst.append(w.text())\n",
    "  return sentence_lst\n",
    "\n",
    "def split_folia(doc):\n",
    "  doc_lst = []\n",
    "  for s in doc.sentences():\n",
    "    s_lst = split_words(s)\n",
    "    doc_lst.append(s_lst)\n",
    "  return doc_lst\n",
    "\n",
    "def select_random_files(path_to_folder, file_extension, num_files):\n",
    "  file_paths = [os.path.join(path_to_folder, f) for f in os.listdir(path_to_folder) if f.endswith(file_extension)]\n",
    "  return random.sample(file_paths, num_files)\n",
    "  \n",
    "def select_all_files(path_to_folder, file_extension):\n",
    "  return [os.path.join(path_to_folder, f) for f in os.listdir(path_to_folder) if f.endswith(file_extension)]\n",
    "\n",
    "\n",
    "def folia_to_df(folia_path):\n",
    "  doc = folia.Document(file=folia_path)\n",
    "  doc_lst = split_folia(doc)\n",
    "  df = pd.DataFrame({'sentence': doc_lst})\n",
    "  df[\"text_id\"] = extract_dbnl_id(folia_path)\n",
    "  return df\n",
    "\n",
    "def extract_dbnl_id(f_name):\n",
    "    \"\"\"\n",
    "    extract dbnl-id from a filename.\n",
    "    \"\"\"\n",
    "    pattern = r\"[a-zA-Z_]{4}\\d{3}[a-zA-Z_\\d]{4}\\d{2}\"\n",
    "    regex = re.compile(pattern)\n",
    "    return regex.search(f_name)[0]\n",
    "\n",
    "def folia_to_df_with_sentence(folia_path):\n",
    "  out_path = f\"dbnl_sentences/{extract_dbnl_id(folia_path)}.pkl\"\n",
    "  if not os.path.exists(out_path):\n",
    "    try:\n",
    "        doc = folia.Document(file=folia_path)\n",
    "        splitted_s_lst = split_folia(doc)\n",
    "        s_lst = split_folia_in_sentences(doc)\n",
    "        df = pd.DataFrame({'sentence': s_lst , 'splitted':splitted_s_lst})\n",
    "        df[\"text_id\"] = extract_dbnl_id(folia_path)\n",
    "        df.to_pickle(out_path)\n",
    "    except:\n",
    "        print(f\"for{folia_path} it didn't work\")\n",
    "  return\n",
    "\n",
    "def split_folia_in_sentences(doc):\n",
    "  doc_lst = []\n",
    "  for s in doc.sentences():\n",
    "    doc_lst.append(s.text())\n",
    "  return doc_lst\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "phEEWsLOAkxY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory dbnl_sentences already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "making df:   0%|          | 0/3415 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fordbnl_folia/_sta001stat02.folia.xml it didn't work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "making df: 100%|██████████| 3415/3415 [01:25<00:00, 39.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fordbnl_folia/_vor003vors01.folia.xml it didn't work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "create_if_absent(\"dbnl_sentences\")\n",
    "folia_files = select_all_files(\"dbnl_folia\", \"folia.xml\")\n",
    "\n",
    "for f in tqdm.tqdm(folia_files, desc = \"making df\"):\n",
    "  folia_to_df_with_sentence(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 things: text in sentence, dutch, footnotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in 'dbnl_xml': 3433\n",
      "Number of files in 'dbnl_txt': 3415\n",
      "Number of files in 'dbnl_folia': 3422\n",
      "Number of files in 'dbnl_sentences': 3413\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_folder(folder_path):\n",
    "    file_count = 0\n",
    "\n",
    "    # Iterate over all the items in the folder\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        file_count += len(files)\n",
    "\n",
    "    return file_count\n",
    "\n",
    "# Provide the path to the folder you want to count files in\n",
    "folder_paths = ['dbnl_xml', 'dbnl_txt', 'dbnl_folia', 'dbnl_sentences']\n",
    "\n",
    "for p in folder_paths:\n",
    "    num_files = count_files_in_folder(p)\n",
    "    print(f\"Number of files in '{p}': {num_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolders1 = [\"towards_big_df/subfolder_1\", \"towards_big_df/subfolder_2\", \"towards_big_df/subfolder_3\", \"towards_big_df/subfolder_4\", \"towards_big_df/subfolder_5\"]\n",
    "subfolders2 = [\"towards_big_df/subfolder_5\", \"towards_big_df/subfolder_6\", \"towards_big_df/subfolder_7\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading pkl's: 100%|██████████| 500/500 [00:10<00:00, 46.04it/s]\n",
      "reading pkl's: 100%|██████████| 500/500 [00:12<00:00, 40.03it/s] \n",
      "reading pkl's: 100%|██████████| 500/500 [00:14<00:00, 35.16it/s]\n",
      "reading pkl's: 100%|██████████| 500/500 [00:15<00:00, 33.32it/s] \n",
      "reading pkl's: 100%|██████████| 500/500 [00:13<00:00, 38.28it/s] "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def select_all_files(path_to_folder, file_extension):\n",
    "  return [os.path.join(path_to_folder, f) for f in os.listdir(path_to_folder) if f.endswith(file_extension)]\n",
    "\n",
    "n = 0\n",
    "for s in subfolders1:\n",
    "    n+=1\n",
    "    paths = select_all_files(s, \".pkl\")\n",
    "    dfs = []\n",
    "    for p in tqdm.tqdm(paths, desc = \"reading pkl's\"):\n",
    "        df = pd.read_pickle(p)\n",
    "        dfs.append(df)\n",
    "    total_df = pd.concat(dfs, ignore_index = True)\n",
    "    total_df.to_pickle(f\"towards_big_df_sub_{n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading pkl's: 100%|██████████| 500/500 [00:14<00:00, 33.75it/s]\n",
      "reading pkl's: 100%|██████████| 500/500 [00:11<00:00, 44.23it/s] \n",
      "reading pkl's: 100%|██████████| 413/413 [00:11<00:00, 36.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def select_all_files(path_to_folder, file_extension):\n",
    "  return [os.path.join(path_to_folder, f) for f in os.listdir(path_to_folder) if f.endswith(file_extension)]\n",
    "\n",
    "n = 5\n",
    "for s in subfolders2:\n",
    "    n+=1\n",
    "    paths = select_all_files(s, \".pkl\")\n",
    "    dfs = []\n",
    "    for p in tqdm.tqdm(paths, desc = \"reading pkl's\"):\n",
    "        df = pd.read_pickle(p)\n",
    "        dfs.append(df)\n",
    "    total_df = pd.concat(dfs, ignore_index = True)\n",
    "    total_df.to_pickle(f\"towards_big_df_sub_{n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the source folder containing the files\n",
    "source_folder = \"dbnl_sentences\"\n",
    "\n",
    "# Define the destination folder for the subfolders\n",
    "destination_folder = \"towards_big_df\"\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all the files in the source folder\n",
    "files = os.listdir(source_folder)\n",
    "\n",
    "# Calculate the number of files per subfolder\n",
    "files_per_subfolder = 500\n",
    "\n",
    "# Calculate the total number of subfolders needed\n",
    "total_subfolders = len(files) // files_per_subfolder + 1\n",
    "\n",
    "# Create the subfolders and move the files into them\n",
    "for i in range(total_subfolders):\n",
    "    # Create a subfolder\n",
    "    subfolder_path = os.path.join(destination_folder, f'subfolder_{i+1}')\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "    \n",
    "    # Move the files into the subfolder\n",
    "    start_index = i * files_per_subfolder\n",
    "    end_index = start_index + files_per_subfolder\n",
    "    for file in files[start_index:end_index]:\n",
    "        file_path = os.path.join(source_folder, file)\n",
    "        shutil.move(file_path, subfolder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = \"towards_big_df_sub_1\"\n",
    "df2 = \"towards_big_df_sub_2\"\n",
    "df3 = \"towards_big_df_sub_3\"\n",
    "df4 = \"towards_big_df_sub_4\"\n",
    "df5 = \"towards_big_df_sub_5\"\n",
    "df6 = \"towards_big_df_sub_6\"\n",
    "df7 = \"towards_big_df_sub_7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fastparquet\n",
    "e = df5\n",
    "f = \"df5\" \n",
    "e_df = pd.read_pickle(e)\n",
    "e_df.to_parquet(f'parquets/{f}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([\n",
    "    pd.read_parquet('parquets/df1.parquet'),\n",
    "    pd.read_parquet('parquets/df2.parquet'),\n",
    "    pd.read_parquet('parquets/df3.parquet'),\n",
    "    pd.read_parquet('parquets/df4.parquet'),\n",
    "    pd.read_parquet('parquets/df5.parquet'),\n",
    "    pd.read_parquet('parquets/df6.parquet'),\n",
    "    pd.read_parquet('parquets/df7.parquet')],\n",
    "    ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fastparquet\n",
    "merged_df = pd.concat([\n",
    "    pd.read_parquet('parquets/df1.parquet'),\n",
    "    pd.read_parquet('parquets/df2.parquet')],\n",
    "    ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_parquet(\"parquets/df1and2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparquets/df1and2.parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(a\u001b[38;5;241m.\u001b[39mtail())\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parquet.py:509\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    506\u001b[0m     use_nullable_dtypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    507\u001b[0m check_dtype_backend(dtype_backend)\n\u001b[0;32m--> 509\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/io/parquet.py:338\u001b[0m, in \u001b[0;36mFastParquetImpl.read\u001b[0;34m(self, path, columns, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m     path \u001b[38;5;241m=\u001b[39m handles\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 338\u001b[0m     parquet_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mParquetFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparquet_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    339\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parquet_file\u001b[38;5;241m.\u001b[39mto_pandas(columns\u001b[38;5;241m=\u001b[39mcolumns, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastparquet/api.py:135\u001b[0m, in \u001b[0;36mParquetFile.__init__\u001b[0;34m(self, fn, verify, open_with, root, sep, fs, pandas_nulls, dtypes)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(fn, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# file-like\u001b[39;00m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_scheme \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mempty\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m    137\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot use file-like input \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    138\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwith multi-file data\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/fastparquet/api.py:225\u001b[0m, in \u001b[0;36mParquetFile._parse_header\u001b[0;34m(self, f, verify)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ParquetException(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMetadata parse failed: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn)\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# for rg in fmd.row_groups:\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m rg \u001b[38;5;129;01min\u001b[39;00m fmd[\u001b[38;5;241m4\u001b[39m]:\n\u001b[1;32m    226\u001b[0m     \u001b[38;5;66;03m# chunks = rg.columns\u001b[39;00m\n\u001b[1;32m    227\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m rg[\u001b[38;5;241m1\u001b[39m]    \n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunks:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "a = pd.read_parquet(\"parquets/df1and2.parquet\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
