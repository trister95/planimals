{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_df = pd.read_csv(\"/home/arjan_v_d/planimals/data/final_batch_0.csv\")\n",
    "sample = sentence_df.sample(n=1000)\n",
    "sentences = sample[\"sentences\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n",
    "env.read_env(\".env\")  # Read .env file\n",
    "OPENAI_API_KEY = env(\"OPENAI_API_KEY\")  # Get the API key\n",
    "OPEN_AI_TOKEN_I_PRICE = (\n",
    "    0.003 / 1000\n",
    ")  # Replace X with the current price per token from OpenAI's pricing\n",
    "OPEN_AI_TOKEN_O_PRICE = 0.006 / 1000\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(sentence, word):\n",
    "    # check if -1 is needed for IOB notation\n",
    "    start_index = sentence.find(word)\n",
    "    end_index = start_index + len(word)\n",
    "    return [start_index, end_index]\n",
    "\n",
    "\n",
    "def has_multiple_occurrences(sentence, model_output):\n",
    "    for category in model_output:\n",
    "        for word in model_output[category]:\n",
    "            if sentence.count(word) > 1:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def transform_output(sentence, model_output):\n",
    "    try:\n",
    "        labels = []\n",
    "        flagged = False\n",
    "\n",
    "        # If multiple occurrences, you should annotate manually\n",
    "        if has_multiple_occurrences(sentence, model_output):\n",
    "            return {\"Sentence\": sentence, \"Labels\": [], \"Flagged\": True}\n",
    "\n",
    "        for category in model_output.keys():\n",
    "            for word in model_output[category]:\n",
    "                indices = find_indices(sentence, word)\n",
    "                if indices:\n",
    "                    labels.append(indices + [category])  # 'animal' or 'plant'\n",
    "\n",
    "        return {\"Sentence\": sentence, \"Labels\": labels, \"Flagged\": flagged}\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print the error message for debugging\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "        # Flag the sentence for manual review\n",
    "        return {\"Sentence\": sentence, \"Labels\": [], \"Flagged\": True}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You'll analyze Old Dutch sentences to identify plants and animals. Given a sentence provide the following fields in a JSON dict: 'plants', 'animals'. Remember: Tag only explicit references to plants or animals. Ignore plant/animal parts, products, and habitats. No tagging of particles. Tag only the nouns that directly refer to the plant or animal, excluding adjectives that are not part of a species' common name or a proper noun. Tag literally (use the exact same spelling as in the Dutch sentence). Text: {x}\"\n",
    ")\n",
    "model = ChatOpenAI(model=\"ft:gpt-3.5-turbo-1106:personal::8KmdqIHA\")\n",
    "map_ = RunnableMap(x=RunnablePassthrough())\n",
    "chain = map_ | prompt | model | SimpleJsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_token_count(text):\n",
    "    return len(str(text)) / 4\n",
    "\n",
    "\n",
    "def process_batch(batch):\n",
    "    input_token_count = 0\n",
    "    output_token_count = 0\n",
    "    results = []\n",
    "\n",
    "    for sentence in batch:\n",
    "        # Count input tokens\n",
    "        input_tokens = estimate_token_count(sentence)\n",
    "        input_token_count += input_tokens\n",
    "\n",
    "        # API call\n",
    "        response = chain.invoke(sentence)\n",
    "\n",
    "        # Count output tokens\n",
    "        output_tokens = estimate_token_count(response)\n",
    "        output_token_count += output_tokens\n",
    "\n",
    "        # Process response\n",
    "        tagged = transform_output(sentence, response)\n",
    "        results.append(tagged)\n",
    "\n",
    "    return results, input_token_count, output_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  58%|█████▊    | 29/50 [03:10<02:17,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches:  80%|████████  | 40/50 [04:25<01:08,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "An error occurred: 'NoneType' object is not iterable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing batches: 100%|██████████| 50/50 [05:28<00:00,  6.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated input cost: 0.11639325\n",
      "Estimated output cost: 0.044907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "batch_size = 20  # Adjust based on your needs and API limitations\n",
    "batches = [sentences[i : i + batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "for batch in tqdm(batches, desc=\"Processing batches\"):\n",
    "    batch_results, batch_input_tokens, batch_output_tokens = process_batch(batch)\n",
    "    results.extend(batch_results)\n",
    "    total_input_tokens += batch_input_tokens\n",
    "    total_output_tokens += batch_output_tokens\n",
    "\n",
    "print(f\"Estimated input cost: {total_input_tokens*OPEN_AI_TOKEN_I_PRICE}\")\n",
    "print(f\"Estimated output cost: {total_output_tokens*OPEN_AI_TOKEN_O_PRICE}\")\n",
    "\n",
    "# Create and save DataFrame\n",
    "df = pd.DataFrame(results, columns=[\"Sentence\", \"Labels\", \"Flagged\"])\n",
    "df.to_csv(\"plants_animals.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
