{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dubbelen eruit\n",
    "#husselen\n",
    "#opknippen in delen, 10.000 per stuk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def process_chunk(dataframes):\n",
    "    # Combine, deduplicate, and shuffle\n",
    "    combined = pd.concat(dataframes)\n",
    "    unique = combined.drop_duplicates()\n",
    "    shuffled = unique.sample(frac=1).reset_index(drop=True)\n",
    "    return shuffled\n",
    "\n",
    "# Assuming all your dataframe names or paths are in a list\n",
    "all_dataframe_paths = ['path_to_df1', 'path_to_df2', ..., 'path_to_df3000']\n",
    "chunk_size = 100  # Adjust this based on your system's capability\n",
    "processed_chunks = []\n",
    "\n",
    "for i in range(0, len(all_dataframe_paths), chunk_size):\n",
    "    # Load chunk of dataframes\n",
    "    chunk_paths = all_dataframe_paths[i:i + chunk_size]\n",
    "    chunk_dfs = [pd.read_csv(path) for path in chunk_paths]\n",
    "\n",
    "    # Process and store the chunk\n",
    "    processed_chunk = process_chunk(chunk_dfs)\n",
    "    processed_chunk.to_csv(f'processed_chunk_{i // chunk_size}.csv', index=False)\n",
    "    processed_chunks.append(f'processed_chunk_{i // chunk_size}.csv')\n",
    "\n",
    "# Finally, combine, shuffle, and batch the processed chunks\n",
    "final_df = pd.concat([pd.read_csv(chunk) for chunk in processed_chunks])\n",
    "final_shuffled_df = final_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "batch_size = 10000\n",
    "for i in range(0, len(final_shuffled_df), batch_size):\n",
    "    batch = final_shuffled_df.iloc[i:i + batch_size]\n",
    "    batch.to_csv(f'final_batch_{i // batch_size}.csv', index=False)\n",
    "\n",
    "# Optionally, clean up intermediate chunk files\n",
    "for chunk in processed_chunks:\n",
    "    os.remove(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#verzamel juiste aantal zinnen\n",
    "#gooi in de api\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import langchain\n",
    "import openai\n",
    "import re\n",
    "from environs import Env\n",
    "from tqdm import tqdm\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableMap, RunnablePassthrough\n",
    "from langchain.output_parsers.json import SimpleJsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env()\n",
    "env.read_env(\".env\")  # Read .env file\n",
    "OPENAI_API_KEY = env(\"OPENAI_API_KEY\")  # Get the API key\n",
    "OPEN_AI_TOKEN_I_PRICE = (\n",
    "    0.003 / 1000\n",
    ")  # Replace X with the current price per token from OpenAI's pricing\n",
    "OPEN_AI_TOKEN_O_PRICE = 0.006 / 1000\n",
    "openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_indices(sentence, word):\n",
    "    # check if -1 is needed for IOB notation\n",
    "    start_index = sentence.find(word)\n",
    "    end_index = start_index + len(word)\n",
    "    return [start_index, end_index]\n",
    "\n",
    "\n",
    "def has_multiple_occurrences(sentence, model_output):\n",
    "    for category in model_output:\n",
    "        for word in model_output[category]:\n",
    "            if sentence.count(word) > 1:\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def transform_output(sentence, model_output):\n",
    "    labels = []\n",
    "    flagged = False\n",
    "\n",
    "    # If multiple occurences, you should annotate manually\n",
    "    if has_multiple_occurrences(sentence, model_output):\n",
    "        return {\"Sentence\": sentence, \"Labels\": [], \"Flagged\": True}\n",
    "\n",
    "    for category in model_output.keys():\n",
    "        for word in model_output[category]:\n",
    "            indices = find_indices(sentence, word)\n",
    "            if indices:\n",
    "                labels.append(indices + [category])  # 'animal' or 'plant'\n",
    "\n",
    "    return {\"Sentence\": sentence, \"Labels\": labels, \"Flagged\": flagged}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"You'll analyze Old Dutch sentences to identify plants and animals. Given a sentence provide the following fields in a JSON dict: 'plants', 'animals'. Remember: Tag only explicit references to plants or animals. Ignore plant/animal parts, products, and habitats. No tagging of particles. Tag only the nouns that directly refer to the plant or animal, excluding adjectives that are not part of a species' common name or a proper noun. Tag literally (use the exact same spelling as in the Dutch sentence). Text: {x}\"\n",
    ")\n",
    "model = ChatOpenAI(model=\"ft:gpt-3.5-turbo-1106:personal::8KmdqIHA\")\n",
    "map_ = RunnableMap(x=RunnablePassthrough())\n",
    "chain = map_ | prompt | model | SimpleJsonOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_token_count(text):\n",
    "    return len(str(text)) / 4\n",
    "\n",
    "\n",
    "def process_batch(batch):\n",
    "    input_token_count = 0\n",
    "    output_token_count = 0\n",
    "    results = []\n",
    "\n",
    "    for sentence in batch:\n",
    "        # Count input tokens\n",
    "        input_tokens = estimate_token_count(sentence)\n",
    "        input_token_count += input_tokens\n",
    "\n",
    "        # API call\n",
    "        response = chain.invoke(sentence)\n",
    "\n",
    "        # Count output tokens\n",
    "        output_tokens = estimate_token_count(response)\n",
    "        output_token_count += output_tokens\n",
    "\n",
    "        # Process response\n",
    "        tagged = transform_output(sentence, response)\n",
    "        results.append(tagged)\n",
    "\n",
    "    return results, input_token_count, output_token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "batch_size = 20  # Adjust based on your needs and API limitations\n",
    "batches = [dutch_lst[i : i + batch_size] for i in range(0, len(dutch_lst), batch_size)]\n",
    "\n",
    "total_input_tokens = 0\n",
    "total_output_tokens = 0\n",
    "for batch in tqdm(batches, desc=\"Processing batches\"):\n",
    "    batch_results, batch_input_tokens, batch_output_tokens = process_batch(batch)\n",
    "    results.extend(batch_results)\n",
    "    total_input_tokens += batch_input_tokens\n",
    "    total_output_tokens += batch_output_tokens\n",
    "\n",
    "print(f\"Estimated input cost: {total_input_tokens*OPEN_AI_TOKEN_I_PRICE}\")\n",
    "print(f\"Estimated output cost: {total_output_tokens*OPEN_AI_TOKEN_O_PRICE}\")\n",
    "\n",
    "# Create and save DataFrame\n",
    "df = pd.DataFrame(results, columns=[\"Sentence\", \"Labels\", \"Flagged\"])\n",
    "df.to_csv(\"plants_animals.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zet resultaat van de api om naar IOB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emanjacavas/GysBERT\")\n",
    "\n",
    "def align_labels_with_tokens(tokenizer, sentence, labels):\n",
    "    tokenized_input = tokenizer(sentence, return_offsets_mapping=True)\n",
    "    token_labels = [\"O\"] * len(tokenized_input[\"input_ids\"])  # Initialize labels as 'Outside'\n",
    "\n",
    "    for start, end, label in labels:\n",
    "        token_start = None\n",
    "        token_end = None\n",
    "\n",
    "        # Find the token indices that correspond to the start and end of the span\n",
    "        for idx, (token_start_pos, token_end_pos) in enumerate(tokenized_input.offset_mapping):\n",
    "            if token_start is None and token_start_pos >= start:\n",
    "                token_start = idx\n",
    "            if token_end_pos <= end:\n",
    "                token_end = idx\n",
    "\n",
    "        # Assign labels (using IOB format)\n",
    "        if token_start is not None and token_end is not None:\n",
    "            token_labels[token_start] = f\"B-{label}\"\n",
    "            for idx in range(token_start + 1, token_end + 1):\n",
    "                token_labels[idx] = f\"I-{label}\"\n",
    "\n",
    "    # Remove the label for special tokens (like [CLS], [SEP])\n",
    "    token_labels = [label if offset_mapping[0] != offset_mapping[1] else \"O\" for label, offset_mapping in zip(token_labels, tokenized_input.offset_mapping)]\n",
    "\n",
    "    return token_labels\n",
    "\n",
    "# Example usage\n",
    "sentence = \"Kat krabt de krullen van de trap\"\n",
    "labels = [[0, 2, \"animal\"]]\n",
    "aligned_labels = align_labels_with_tokens(tokenizer, sentence, labels)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
