{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8eNUNUYj-w0d"
   },
   "source": [
    "## Clone, install, import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7wJk2cZN9lf-",
    "outputId": "d8c74583-e407-46f7-81f5-7ddb2e9e8927",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!git clone #add link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxMAqL-Z-_2G",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I9ThYhJj-6Ja"
   },
   "outputs": [],
   "source": [
    "from utils import create_if_absent, download_and_unzip\n",
    "\n",
    "create_if_absent(\"../data/dbnl_xml\")\n",
    "download_and_unzip(url = \"https://www.dbnl.org/letterkunde/pd/xml_pd.zip\", output_dir = \"../data/dbnl_xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-UAHKveR_KtO"
   },
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "98TjeKar_GKR"
   },
   "outputs": [],
   "source": [
    "from preprocessing import make_html_entity_dict, additional_declaration_str, add_declaration_to_xml\n",
    "\n",
    "html_to_decimal_dict = make_html_entity_dict()\n",
    "addition =  additional_declaration_str(html_to_decimal_dict, '.dtd\"')\n",
    "add_declaration_to_xml('dbnl_xml', '<!DOCTYPE', '.dtd\"', addition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "id": "HcWUpuin_Oqn",
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory dbnl_txt created\n",
      "file teen002bijz01 could not be parsed\n",
      "file hugo001piad01 could not be parsed\n",
      "file vaen001quin01 could not be parsed\n",
      "file vry_001anat01 could not be parsed\n",
      "file _gid001183901 could not be parsed\n",
      "file luyk001jezu01 could not be parsed\n",
      "file aa__001biog02 could not be parsed\n",
      "file herl001besc01 could not be parsed\n",
      "file lede003drie01 could not be parsed\n",
      "file elge001zinn01 could not be parsed\n",
      "file ferm001nieu01 could not be parsed\n",
      "file leuv001amor01 could not be parsed\n",
      "file aa__001biog11 could not be parsed\n",
      "file carp002verh01 could not be parsed\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import dbnl_to_txt\n",
    "\n",
    "create_if_absent(\"dbnl_txt\")\n",
    "dbnl_to_txt(\"dbnl_xml\", \"dbnl_txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "E3OwXGIp_hDZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory dbnl_folia already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "looping through text files: 100%|██████████| 3415/3415 [38:02<00:00,  1.50it/s]  \n"
     ]
    }
   ],
   "source": [
    "from preprocessing import txt_to_folia\n",
    "\n",
    "create_if_absent(\"dbnl_folia\")\n",
    "txt_to_folia(\"dbnl_txt\", \"dbnl_folia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "phEEWsLOAkxY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory dbnl_sentences already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "making df:   0%|          | 0/3415 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fordbnl_folia/_sta001stat02.folia.xml it didn't work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "making df: 100%|██████████| 3415/3415 [01:25<00:00, 39.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fordbnl_folia/_vor003vors01.folia.xml it didn't work\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import select_all_files, folia_to_df\n",
    "\n",
    "create_if_absent(\"dbnl_sentences\")\n",
    "folia_files = select_all_files(\"dbnl_folia\", \"folia.xml\")\n",
    "\n",
    "for f in tqdm.tqdm(folia_files, desc = \"making df\"):\n",
    "  folia_to_df(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 things: text in sentence, dutch, footnotes. Add that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in 'dbnl_xml': 3433\n",
      "Number of files in 'dbnl_txt': 3415\n",
      "Number of files in 'dbnl_folia': 3422\n",
      "Number of files in 'dbnl_sentences': 3413\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_folder(folder_path):\n",
    "    file_count = 0\n",
    "\n",
    "    # Iterate over all the items in the folder\n",
    "    for _, _, files in os.walk(folder_path):\n",
    "        file_count += len(files)\n",
    "\n",
    "    return file_count\n",
    "\n",
    "# Provide the path to the folder you want to count files in\n",
    "folder_paths = ['dbnl_xml', 'dbnl_txt', 'dbnl_folia', 'dbnl_sentences']\n",
    "\n",
    "for p in folder_paths:\n",
    "    num_files = count_files_in_folder(p)\n",
    "    print(f\"Number of files in '{p}': {num_files}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolders1 = [\"towards_big_df/subfolder_1\", \"towards_big_df/subfolder_2\", \"towards_big_df/subfolder_3\", \"towards_big_df/subfolder_4\", \"towards_big_df/subfolder_5\"]\n",
    "subfolders2 = [\"towards_big_df/subfolder_5\", \"towards_big_df/subfolder_6\", \"towards_big_df/subfolder_7\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading pkl's: 100%|██████████| 500/500 [00:10<00:00, 46.04it/s]\n",
      "reading pkl's: 100%|██████████| 500/500 [00:12<00:00, 40.03it/s] \n",
      "reading pkl's: 100%|██████████| 500/500 [00:14<00:00, 35.16it/s]\n",
      "reading pkl's: 100%|██████████| 500/500 [00:15<00:00, 33.32it/s] \n",
      "reading pkl's: 100%|██████████| 500/500 [00:13<00:00, 38.28it/s] "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def select_all_files(path_to_folder, file_extension):\n",
    "  return [os.path.join(path_to_folder, f) for f in os.listdir(path_to_folder) if f.endswith(file_extension)]\n",
    "\n",
    "n = 0\n",
    "for s in subfolders1:\n",
    "    n+=1\n",
    "    paths = select_all_files(s, \".pkl\")\n",
    "    dfs = []\n",
    "    for p in tqdm.tqdm(paths, desc = \"reading pkl's\"):\n",
    "        df = pd.read_pickle(p)\n",
    "        dfs.append(df)\n",
    "    total_df = pd.concat(dfs, ignore_index = True)\n",
    "    total_df.to_pickle(f\"towards_big_df_sub_{n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "reading pkl's: 100%|██████████| 500/500 [00:14<00:00, 33.75it/s]\n",
      "reading pkl's: 100%|██████████| 500/500 [00:11<00:00, 44.23it/s] \n",
      "reading pkl's: 100%|██████████| 413/413 [00:11<00:00, 36.02it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "def select_all_files(path_to_folder, file_extension):\n",
    "  return [os.path.join(path_to_folder, f) for f in os.listdir(path_to_folder) if f.endswith(file_extension)]\n",
    "\n",
    "n = 5\n",
    "for s in subfolders2:\n",
    "    n+=1\n",
    "    paths = select_all_files(s, \".pkl\")\n",
    "    dfs = []\n",
    "    for p in tqdm.tqdm(paths, desc = \"reading pkl's\"):\n",
    "        df = pd.read_pickle(p)\n",
    "        dfs.append(df)\n",
    "    total_df = pd.concat(dfs, ignore_index = True)\n",
    "    total_df.to_pickle(f\"towards_big_df_sub_{n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "# Define the source folder containing the files\n",
    "source_folder = \"dbnl_sentences\"\n",
    "\n",
    "# Define the destination folder for the subfolders\n",
    "destination_folder = \"towards_big_df\"\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "os.makedirs(destination_folder, exist_ok=True)\n",
    "\n",
    "# Get a list of all the files in the source folder\n",
    "files = os.listdir(source_folder)\n",
    "\n",
    "# Calculate the number of files per subfolder\n",
    "files_per_subfolder = 500\n",
    "\n",
    "# Calculate the total number of subfolders needed\n",
    "total_subfolders = len(files) // files_per_subfolder + 1\n",
    "\n",
    "# Create the subfolders and move the files into them\n",
    "for i in range(total_subfolders):\n",
    "    # Create a subfolder\n",
    "    subfolder_path = os.path.join(destination_folder, f'subfolder_{i+1}')\n",
    "    os.makedirs(subfolder_path, exist_ok=True)\n",
    "    \n",
    "    # Move the files into the subfolder\n",
    "    start_index = i * files_per_subfolder\n",
    "    end_index = start_index + files_per_subfolder\n",
    "    for file in files[start_index:end_index]:\n",
    "        file_path = os.path.join(source_folder, file)\n",
    "        shutil.move(file_path, subfolder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = \"towards_big_df_sub_1\"\n",
    "df2 = \"towards_big_df_sub_2\"\n",
    "df3 = \"towards_big_df_sub_3\"\n",
    "df4 = \"towards_big_df_sub_4\"\n",
    "df5 = \"towards_big_df_sub_5\"\n",
    "df6 = \"towards_big_df_sub_6\"\n",
    "df7 = \"towards_big_df_sub_7\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fastparquet\n",
    "e = df5\n",
    "f = \"df5\" \n",
    "e_df = pd.read_pickle(e)\n",
    "e_df.to_parquet(f'parquets/{f}.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([\n",
    "    pd.read_parquet('parquets/df1.parquet'),\n",
    "    pd.read_parquet('parquets/df2.parquet'),\n",
    "    pd.read_parquet('parquets/df3.parquet'),\n",
    "    pd.read_parquet('parquets/df4.parquet'),\n",
    "    pd.read_parquet('parquets/df5.parquet'),\n",
    "    pd.read_parquet('parquets/df6.parquet'),\n",
    "    pd.read_parquet('parquets/df7.parquet')],\n",
    "    ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fastparquet\n",
    "merged_df = pd.concat([\n",
    "    pd.read_parquet('parquets/df1.parquet'),\n",
    "    pd.read_parquet('parquets/df2.parquet')],\n",
    "    ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_parquet(\"parquets/df1and2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "a = pd.read_parquet(\"parquets/df1and2.parquet\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
