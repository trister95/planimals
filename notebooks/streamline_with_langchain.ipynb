{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-11 16:22:15.724236: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-01-11 16:22:16.814537: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "%run ../src/utils.py\n",
    "%run ../src/llm_annot_makeover.py\n",
    "%run ../src/disagreement_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environs import Env\n",
    "import openai\n",
    "\n",
    "env = Env()\n",
    "env.read_env(\".env\")  # Read .env file\n",
    "OPENAI_API_KEY = env(\"OPENAI_API_KEY\")  # Get the API key\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "HF_TOKEN = env(\"HUGGINGFACE_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now at 3, 4, and 5 (out of 20) of the 1st batch (out of 75). Together, that's 15000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_3rdbatch = pd.read_csv(\"/home/arjan_v_d/planimals/data/dbnl_sentences/batch_0_in_20_parts/df_3_part.csv\")\n",
    "df_4rdbatch = pd.read_csv(\"/home/arjan_v_d/planimals/data/dbnl_sentences/batch_0_in_20_parts/df_4_part.csv\")\n",
    "df_5rdbatch = pd.read_csv(\"/home/arjan_v_d/planimals/data/dbnl_sentences/batch_0_in_20_parts/df_5_part.csv\")\n",
    "sentences3 = df_3rdbatch[\"sentences\"].to_list()\n",
    "sentences4 = df_4rdbatch[\"sentences\"].to_list()\n",
    "sentences5 = df_5rdbatch[\"sentences\"].to_list()\n",
    "\n",
    "sentences = sentences3 + sentences4 + sentences5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 150/150 [18:37<00:00,  7.45s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You'll analyze Old Dutch sentences to identify plants and animals. Given a sentence provide the following fields in a JSON dict: 'plants',\n",
    "'animals'. Remember: Tag only explicit references to plants or animals. Ignore plant/animal parts, products, and habitats. No tagging of\n",
    "particles. Tag only the nouns that directly refer to the plant or animal, excluding adjectives that are not part of a species' common \n",
    "name or a proper noun. Tag literally (use the exact same spelling as in the Dutch sentence). Here's the text:\n",
    "\"\"\"    \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([(\"system\", system_prompt), (\"user\", \"{sentence}\")])\n",
    "model = ChatOpenAI(model=\"ft:gpt-3.5-turbo-1106:personal::8KmdqIHA\")\n",
    "parser = PydanticOutputParser(pydantic_object=Plants_and_animals)\n",
    "chain = prompt | model | parser\n",
    "batch_size = 100  # Adjust based on needs and API limitations\n",
    "batches = [sentences[i : i + batch_size] for i in range(0, len(sentences), batch_size)]\n",
    "\n",
    "results = await process_batches(batches, chain)\n",
    "results_dicts = [annotation.to_dict() for annotation in results]\n",
    "\n",
    "df = pd.DataFrame(results_dicts)\n",
    "df.to_csv(\"gpt3_5_15000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Sentences:   0%|          | 0/15000 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Processing Sentences: 100%|██████████| 15000/15000 [17:34<00:00, 14.22it/s]\n"
     ]
    }
   ],
   "source": [
    "%run ../src/disagreement_analysis.py\n",
    "#df = pd.read_csv(\"../data/llm_annotation/disagreement_analysis/round2/output_llm.csv\")\n",
    "df = custom_model_annotation(df, model_name = \"ArjanvD95/munchhausen_v2_gysb2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"double_llmed_15000.csv\", index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compare_annotations_as_strings(df, llm_col = \"label\", ner_col = \"huggingface_labels\")\n",
    "df.to_csv(\"../data/llm_annotation/disagreement_analysis/round2/llm_and_ner_combination_round2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4705\n"
     ]
    }
   ],
   "source": [
    "same_result_count = sum(data['label'] == data['huggingface_labels'])\n",
    "print(same_result_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same result count: 4705\n",
      "LLM found more named entities: 109\n",
      "HuggingFace model found more named entities: 173\n",
      "Disagreements: 306\n",
      "Flagged sentences: 33\n",
      "HF more minus flagged: 140\n",
      "LLM named entities count: 317\n",
      "HF named entities count: 381\n",
      "LLM & HF found entities and agreed: 166\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"../data/llm_annotation/disagreement_analysis/round2/llm_and_ner_combination_round2.csv\")\n",
    "\n",
    "# Convert the 'label' and 'huggingface_labels' columns from string to actual lists\n",
    "data['label'] = data['label'].fillna('[]').apply(ast.literal_eval)\n",
    "data['huggingface_labels'] = data['huggingface_labels'].fillna('[]').apply(ast.literal_eval)\n",
    "\n",
    "# Calculations\n",
    "# 1. How often they had the same result\n",
    "same_result_count = sum(data['label'] == data['huggingface_labels'])\n",
    "\n",
    "# 2. How often the LLM found more named entities than the HuggingFace model\n",
    "llm_more_count = sum(len(llm) > len(hf) for llm, hf in zip(data['label'], data['huggingface_labels']))\n",
    "\n",
    "# 3. How often the HuggingFace model found more named entities than the LLM\n",
    "hf_more_count = sum(len(hf) > len(llm) for llm, hf in zip(data['label'], data['huggingface_labels']))\n",
    "\n",
    "# 4. How many disagreements were found\n",
    "disagreement_count = data['disagreement'].sum()\n",
    "\n",
    "# 5. How many sentences were flagged\n",
    "flagged_count = data['flagged'].sum()\n",
    "\n",
    "# 6. The number of sentences where the HuggingFace model found more named entities minus the number of flagged sentences\n",
    "hf_more_minus_flagged = hf_more_count - flagged_count\n",
    "\n",
    "# 7. In how many sentences did the LLM find named entities?\n",
    "llm_named_entities_count = sum(len(llm) > 0 for llm in data['label'])\n",
    "\n",
    "# 8. In how many sentences did the HuggingFace model find named entities?\n",
    "hf_named_entities_count = sum(len(hf) > 0 for hf in data['huggingface_labels'])\n",
    "\n",
    "#9. When both the hf model and the llm found entities in a sentence, how often did they agree?\n",
    "both_found_and_agreed_count = sum(len(llm) > 0 and len(hf) > 0 and llm == hf for llm, hf in zip(data['label'], data['huggingface_labels']))\n",
    "\n",
    "# Results\n",
    "print('Same result count:', same_result_count)\n",
    "print('LLM found more named entities:', llm_more_count)\n",
    "print('HuggingFace model found more named entities:', hf_more_count)\n",
    "print('Disagreements:', disagreement_count)\n",
    "print('Flagged sentences:', flagged_count)\n",
    "print('HF more minus flagged:', hf_more_minus_flagged)\n",
    "print('LLM named entities count:', llm_named_entities_count)\n",
    "print('HF named entities count:', hf_named_entities_count)\n",
    "print('LLM & HF found entities and agreed:', both_found_and_agreed_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Manually check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def dataframe_columns_to_jsonl(df, text_column_name, label_column_name, output_file):\n",
    "    \"\"\"\n",
    "    Convert specified columns of a DataFrame into a JSONL file.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the data.\n",
    "    text_column_name (str): The name of the text column to convert.\n",
    "    label_column_name (str): The name of the label column.\n",
    "    output_file (str): The name of the output JSONL file.\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w') as file:\n",
    "        for text_item, label_item in zip(df[text_column_name], df[label_column_name]):\n",
    "            # Each item in the text column is a JSON object with corresponding label\n",
    "            json_record = json.dumps({\"text\": text_item, \"label\": label_item})\n",
    "            file.write(json_record + '\\n')\n",
    "    print(f\"File '{output_file}' created successfully.\")\n",
    "\n",
    "def merge_lists_without_duplicates(list1, list2):\n",
    "    # Convert inner lists to tuples for set operations\n",
    "    set1 = set(tuple(item) for item in list1)\n",
    "    set2 = set(tuple(item) for item in list2)\n",
    "\n",
    "    # Merge sets and convert back to list of lists\n",
    "    merged_set = set1.union(set2)\n",
    "    return [list(item) for item in merged_set]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File '../data/llm_annotation/disagreement_analysis/round2/disagreements_3rd_batch_combined_labels.jsonl' created successfully.\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/llm_annotation/disagreement_analysis/round2/llm_and_ner_combination_round2.csv\")\n",
    "df_disagreement = df[df[\"disagreement\"]==True].copy()\n",
    "df_disagreement[\"huggingface_labels\"] =df_disagreement[\"huggingface_labels\"].fillna(\"[]\").apply(ast.literal_eval)\n",
    "df_disagreement[\"label\"] =df_disagreement[\"label\"].fillna(\"[]\").apply(ast.literal_eval)\n",
    "\n",
    "df_disagreement['merged_labels'] = df_disagreement.apply(lambda row: merge_lists_without_duplicates(row['label'], row['huggingface_labels']), axis=1)\n",
    "dataframe_columns_to_jsonl(df_disagreement, 'sentence', 'merged_labels', \"../data/llm_annotation/disagreement_analysis/round2/disagreements_3rd_batch_combined_labels.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update the df after the annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in\n",
    "path_to_doccano_annotations = \"../data/llm_annotation/disagreement_analysis/round2/disagreement_solved_batch2.jsonl\"\n",
    "updated_df = update_dataframe_with_annotations(df, path_to_doccano_annotations, 'sentence', 'label', 'flagged') #check if labels are the sames still"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_df.to_csv(\"../data/llm_annotation/disagreement_analysis/round2/5000_sentences_after_manual.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT4 to solve disagreements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = compare_annotations_as_strings(df, llm_col = \"gpt_labels\", ner_col = \"huggingface_labels\")\n",
    "disagreements= df.loc[df[\"disagreement\"]==True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"double_llmed_15000.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "import ast\n",
    "\n",
    "class Disagreements(BaseModel):\n",
    "    \"\"\"Disagreement of two methods on the plants and animals present.\"\"\"\n",
    "    sentence: str = Field(default=None, description=\"The sentence itself\")\n",
    "    gpt_plants: List[str] = Field(description=\"The plants present in the sentence according to gpt\")\n",
    "    gpt_animals: List[str] = Field(description=\"The animals present in the sentence according to gpt\")\n",
    "    bert_plants: List[str] = Field(description=\"The plants present in the sentence according to bert\")\n",
    "    bert_animals: List[str] = Field(description=\"The animals present in the sentence according to bert\")\n",
    "\n",
    "    @classmethod\n",
    "    def from_dataframe_row(cls, row):\n",
    "        def extract_entities(sentence, label_str):\n",
    "            label_str = str(label_str)\n",
    "            entities = {'plants': [], 'animals': []}\n",
    "            # Handle NaN values\n",
    "            if pd.isna(label_str):\n",
    "                return entities\n",
    "\n",
    "            # Convert string representation of list to actual list\n",
    "            label_list = ast.literal_eval(label_str)\n",
    "\n",
    "            for start, end, label in label_list:\n",
    "                # Extract the word from the sentence using span indices\n",
    "                entity = sentence[start:end]\n",
    "                if label == 'plants':\n",
    "                    entities['plants'].append(entity)\n",
    "                elif label == 'animals':\n",
    "                    entities['animals'].append(entity)\n",
    "            \n",
    "            return entities\n",
    "\n",
    "        gpt_entities = extract_entities(row['sentence'], row['gpt_labels'])\n",
    "        bert_entities = extract_entities(row['sentence'], row['huggingface_labels'])\n",
    "\n",
    "        return cls(\n",
    "            sentence=row['sentence'],\n",
    "            gpt_plants=gpt_entities['plants'],\n",
    "            gpt_animals=gpt_entities['animals'],\n",
    "            bert_plants=bert_entities['plants'],\n",
    "            bert_animals=bert_entities['animals']\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "disagreements_list = [Disagreements.from_dataframe_row(row) for _, row in disagreements.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def process_sentence_async(disagreement, chain, limiter):\n",
    "    async with limiter:\n",
    "        try:\n",
    "            model_output = await chain.ainvoke({\"sentence\": disagreement.sentence, \"gpt_plants\":disagreement.gpt_plants, \"gpt_animals\":disagreement.gpt_animals,\n",
    "                                                \"bert_plants\":disagreement.bert_plants, \"bert_animals\":disagreement.bert_animals})\n",
    "            model_output.sentence = disagreement.sentence\n",
    "            return Annotations.create_from_plants_and_animals(model_output)\n",
    "        except Exception as e:\n",
    "            # Handle any exception\n",
    "            return Annotations(\n",
    "                sentence=disagreement.sentence,\n",
    "                gpt_labels = [],\n",
    "                warning=True,\n",
    "                log=f\"Exception: {e}\",\n",
    "            )\n",
    "\n",
    "async def process_batch_llm_async(batch, chain, limiter):\n",
    "    return await asyncio.gather(*[process_sentence_async(disagreement, chain, limiter) for disagreement in batch])\n",
    "\n",
    "\n",
    "async def process_batches(batches, chain, requests_per_minute=900):\n",
    "    limiter = AsyncLimiter(requests_per_minute)\n",
    "    results = []\n",
    "    # Create a tqdm progress bar\n",
    "    with tqdm(total=len(batches), desc=\"Processing Batches\") as pbar:\n",
    "        for batch in batches:\n",
    "            batch_results = await process_batch_llm_async(batch, chain, limiter)\n",
    "            results.extend(batch_results)\n",
    "            pbar.update(1)  # Update the progress bar after each batch\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches:   0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Batches: 100%|██████████| 13/13 [03:11<00:00, 14.70s/it]\n"
     ]
    }
   ],
   "source": [
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a skilled assistant helping with labeling data on the presence of plants and animals \n",
    "in Old Dutch sentences. Analyze the text and think carefully what the correct \n",
    "annotation should be. You do not have to pick one of the proposed annotations, you can also make a decision\n",
    "that differs from both proposed annotations. Here are the text and the proposed annotations.\n",
    "\n",
    "Sentence: {sentence}\n",
    "Model1 found the following plants: {gpt_plants}, and the following animals: {gpt_animals}.\n",
    "Model2 found the following plants: {bert_plants}, and the following animals: {bert_animals}.\n",
    "Provide your annotation in the format: {{\"plants\": [plants], \"animals\": [animals]}}.\n",
    "Do only output the annotation.                               \n",
    "\n",
    "\"\"\")\n",
    "model = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "parser = PydanticOutputParser(pydantic_object=Plants_and_animals)\n",
    "chain = prompt | model | parser\n",
    "\n",
    "batch_size = 100  # Adjust based on needs and API limitations\n",
    "batches = [disagreements_list[i : i + batch_size] for i in range(0, len(disagreements_list), batch_size)]\n",
    "\n",
    "results = await process_batches(batches, chain)\n",
    "results_dicts = [annotation.to_dict() for annotation in results]\n",
    "\n",
    "df = pd.DataFrame(results_dicts)\n",
    "df.to_csv(\"disagreements_solved_15000.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframes(df1, df2):\n",
    "    \"\"\"\n",
    "    Processes the two df's.\n",
    "\n",
    "    Args:\n",
    "    df1 (DataFrame): The 'double_llmed_15000' dataframe.\n",
    "    df2 (DataFrame): The 'disagreements_solved_15000' dataframe.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: Processed 'double_llmed_15000' dataframe.\n",
    "    \"\"\"\n",
    "\n",
    "    # Add a new column 'label_def' and initialize 'log' and 'warning' columns\n",
    "    df1['label_def'] = \"\"\n",
    "    df1['log'] = \"\"\n",
    "    df1['warning'] = False\n",
    "\n",
    "    # Iterate through each row in df1\n",
    "    for index, row in df1.iterrows():\n",
    "        if not row['disagreement']:  # No disagreement\n",
    "            df1.at[index, 'label_def'] = row['huggingface_labels']\n",
    "        else:  # Disagreement exists\n",
    "            # Find the corresponding sentence in df2\n",
    "            match = df2[df2['sentence'] == row['sentence']]\n",
    "            if not match.empty:\n",
    "                if match.iloc[0]['warning']:  # If warning is true\n",
    "                    df1.at[index, 'label_def'] = \"\"\n",
    "                    df1.at[index, 'warning'] = True\n",
    "                    df1.at[index, 'log'] = match.iloc[0]['log']\n",
    "                else:  # No warning\n",
    "                    df1.at[index, 'label_def'] = match.iloc[0]['gpt_labels']\n",
    "\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"double_llmed_15000.csv\")\n",
    "df2 = pd.read_csv(\"disagreements_solved_15000.csv\")\n",
    "integrated_df = process_dataframes(df1, df2)\n",
    "integrated_df.to_csv(\"decision_integrated_15000.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_slim = integrated_df[[\"sentence\", \"warning\", \"label_def\"]]\n",
    "df_no_warnings = df_slim[df_slim[\"warning\"]==False]\n",
    "positives = df_no_warnings[df_no_warnings[\"label_def\"]!= \"[]\"]\n",
    "negatives = df_no_warnings[df_no_warnings[\"label_def\"]== \"[]\"]\n",
    "sample_negatives = negatives.sample(n=len(positives))\n",
    "training_data = pd.concat([positives, sample_negatives])\n",
    "training_data = training_data.sample(frac=1).reset_index(drop = True)\n",
    "training_data.rename(columns={'label_def': 'label'}, inplace=True)\n",
    "training_data = training_data.drop(columns =[\"warning\"])\n",
    "training_data.to_csv(\"trainingdata_from_15000_sentences.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To IOB-notation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../src/llm_annotation.py\n",
    "tag2id = {\n",
    "    \"O\": 0,\n",
    "    \"B-animals\": 1,\n",
    "    \"I-animals\": 2,\n",
    "    \"B-plants\": 3,\n",
    "    \"I-plants\": 4\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv(\"trainingdata_from_15000_sentences.csv\")\n",
    "training_data  = training_data.drop(columns = [\"Unnamed: 0\"])\n",
    "training_data[['sentence', 'label']] = training_data.apply(lambda row: process_row(row, tag2id), axis=1, result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data.to_csv(\"trainingdata_from_15000_sentences_iob.csv\", index =False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
